{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPngFYmuYb49gmevBzTvW9i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vruddhis/semanticshift/blob/main/comparing_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downgrade pandas to what Colab and dask-cudf expect\n",
        "!pip install pandas==2.2.2 --quiet\n",
        "\n",
        "# Downgrade transformers to match sentence-transformers\n",
        "!pip install transformers==4.41.0 --quiet\n",
        "\n",
        "# Downgrade torch to match torchvision/torchaudio/fastai\n",
        "!pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAzkjn99abFX",
        "outputId": "14aba5c4-49bf-4d90-c33c-00c4f546589d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m703.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n"
      ],
      "metadata": {
        "id": "g17hzX889Hbo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", file_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "xmg8YuOT9Ls0",
        "outputId": "8a0eff1b-badc-4406-f28e-94de90b9807b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a71f5d05-112f-4808-b52c-cc0a3db0e0f4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a71f5d05-112f-4808-b52c-cc0a3db0e0f4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.data.jl to train.data.jl\n",
            "Uploaded: train.data.jl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = []\n",
        "with open(file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        data.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(data)} instances\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z37cXQJ4gjCs",
        "outputId": "709684fd-ddf1-4880-8dc4-819fde06168c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1428 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjZ5RgZDf1li",
        "outputId": "a2288c23-2e5d-4e43-a7a0-975dac30c74f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_embedding(text, word, token_idx=None, char_start=None, char_end=None):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
        "    offsets = inputs.pop(\"offset_mapping\")[0]\n",
        "    tokens = inputs[\"input_ids\"]\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    hidden_states = outputs.last_hidden_state[0].cpu()\n",
        "\n",
        "    if char_start is not None and char_end is not None:\n",
        "        target_indices = [i for i, (s, e) in enumerate(offsets) if s <= char_start < e or s < char_end <= e]\n",
        "    elif token_idx is not None:\n",
        "        target_indices = [token_idx]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    if len(target_indices) == 0:\n",
        "        return None\n",
        "\n",
        "    emb = hidden_states[target_indices].mean(dim=0)\n",
        "    return emb.numpy()\n"
      ],
      "metadata": {
        "id": "J1c-5EC7f3j2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for item in tqdm(data):\n",
        "    try:\n",
        "        w = item[\"word\"]\n",
        "        t1, t2 = item[\"tweet1\"], item[\"tweet2\"]\n",
        "\n",
        "        emb1 = get_target_embedding(\n",
        "            t1[\"text\"], w,\n",
        "            token_idx=t1.get(\"token_idx\"),\n",
        "            char_start=t1.get(\"text_start\"),\n",
        "            char_end=t1.get(\"text_end\"),\n",
        "        )\n",
        "        emb2 = get_target_embedding(\n",
        "            t2[\"text\"], w,\n",
        "            token_idx=t2.get(\"token_idx\"),\n",
        "            char_start=t2.get(\"text_start\"),\n",
        "            char_end=t2.get(\"text_end\"),\n",
        "        )\n",
        "\n",
        "        if emb1 is not None and emb2 is not None:\n",
        "            rows.append({\n",
        "                \"word\": w,\n",
        "                \"text1\": t1[\"text\"],\n",
        "                \"text2\": t2[\"text\"],\n",
        "                \"date1\": t1[\"date\"],\n",
        "                \"date2\": t2[\"date\"],\n",
        "                \"emb1\": emb1,\n",
        "                \"emb2\": emb2\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {item['id']}: {e}\")\n",
        "        continue\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM2lXJU8f6JO",
        "outputId": "72d41a0a-7e4c-4ed3-aacb-d5ec8a49b3e3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1428/1428 [11:25<00:00,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    word                                              text1  \\\n",
            "0  frisk  my new most wanted character in smash is frisk...   \n",
            "1  frisk  imagine seeing qoute from cave story making it...   \n",
            "2  frisk  frisk and sans are my two favorite undertale c...   \n",
            "3  frisk  We don't like the search and frisk so this bit...   \n",
            "4  frisk  Hey guys I'm wondering if anybody would draw m...   \n",
            "\n",
            "                                               text2    date1    date2  \\\n",
            "0  I was surprised by how much applause Bloomberg...  2019-02  2020-02   \n",
            "1  Bloomberg? Are you people for real?16 cases of...  2019-02  2020-02   \n",
            "2  Today, in my wrongful convictions class we lis...  2019-02  2020-02   \n",
            "3  who the fuck is listening to mike bloomberg ra...  2019-02  2020-02   \n",
            "4  How about stop in frisk happened in mostly min...  2019-02  2020-02   \n",
            "\n",
            "                                                emb1  \\\n",
            "0  [-0.41409454, -1.0239263, 0.96580446, 0.138374...   \n",
            "1  [0.07960042, -0.688802, 0.5348002, 0.011716709...   \n",
            "2  [-0.1897313, -0.42527908, 0.49965382, 0.217784...   \n",
            "3  [-0.18939906, -0.36904234, 0.5704371, 0.061667...   \n",
            "4  [-0.42097756, -0.6112468, 0.58018327, 0.020720...   \n",
            "\n",
            "                                                emb2  \n",
            "0  [0.028239734, -0.3266933, 0.52268136, -0.02895...  \n",
            "1  [0.28983107, -1.0210311, 0.39893767, 0.1910165...  \n",
            "2  [0.28042075, -0.6890709, 0.24599215, 0.0846705...  \n",
            "3  [0.46234065, -0.2986717, 0.61056906, -0.164837...  \n",
            "4  [-0.1912492, -1.0436229, 0.6294067, 0.48735732...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install POT\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwtmNvDZ3RPM",
        "outputId": "c600e176-a58a-4f44-9c85-27a7364a8f10"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: POT in /usr/local/lib/python3.12/dist-packages (0.9.6.post1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.12/dist-packages (from POT) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.12/dist-packages (from POT) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_sim(emb1, emb2):\n",
        "    if emb1 is None or emb2 is None:\n",
        "        return None\n",
        "    return cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
        "\n",
        "df[\"cosine_similarity\"] = df.apply(lambda row: cosine_sim(row[\"emb1\"], row[\"emb2\"]), axis=1)\n"
      ],
      "metadata": {
        "id": "KO6Mny_R8r_d"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def cosine_sim(emb1, emb2):\n",
        "    if emb1 is None or emb2 is None:\n",
        "        return None\n",
        "    return cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
        "\n",
        "def nn_overlap(embeddings_t1, embeddings_t2):\n",
        "    c1 = np.mean(embeddings_t1, axis=0)\n",
        "    c2 = np.mean(embeddings_t2, axis=0)\n",
        "    return cosine_similarity(c1.reshape(1, -1), c2.reshape(1, -1))[0][0]\n",
        "\n",
        "def centroid_shift(embeddings_t1, embeddings_t2):\n",
        "    c1 = np.mean(embeddings_t1, axis=0)\n",
        "    c2 = np.mean(embeddings_t2, axis=0)\n",
        "    return 1 - cosine_similarity(c1.reshape(1, -1), c2.reshape(1, -1))[0][0]\n",
        "\n",
        "def dispersion_change(embeddings_t1, embeddings_t2):\n",
        "    def disp(embs):\n",
        "        c = np.mean(embs, axis=0)\n",
        "        return np.mean([np.linalg.norm(e - c) for e in embs])\n",
        "    return disp(embeddings_t2) - disp(embeddings_t1)\n",
        "\n",
        "def temporal_ref_similarity(embeddings_t1, embeddings_t2):\n",
        "    sims = [cosine_similarity(e1.reshape(1, -1), e2.reshape(1, -1))[0][0]\n",
        "            for e1 in embeddings_t1 for e2 in embeddings_t2]\n",
        "    return np.mean(sims)\n",
        "\n",
        "def contextual_entropy(texts):\n",
        "    counts = Counter(texts)\n",
        "    probs = np.array(list(counts.values())) / sum(counts.values())\n",
        "    return -np.sum(probs * np.log2(probs + 1e-12))\n",
        "\n",
        "def uot_shift(embeddings_t1, embeddings_t2):\n",
        "    X = np.vstack(embeddings_t1)\n",
        "    Y = np.vstack(embeddings_t2)\n",
        "    a = np.ones(len(X)) / len(X)\n",
        "    b = np.ones(len(Y)) / len(Y)\n",
        "    M = ot.dist(X, Y, metric='cosine')\n",
        "    G = ot.unbalanced.sinkhorn_unbalanced(a, b, M, reg=0.01, reg_m=0.1)\n",
        "    return np.sum(G * M)\n",
        "\n",
        "\n",
        "grouped = df.groupby(\"word\").agg({\n",
        "    \"emb1\": list,\n",
        "    \"emb2\": list,\n",
        "    \"text1\": list,\n",
        "    \"text2\": list,\n",
        "    \"cosine_similarity\": list\n",
        "})\n",
        "\n",
        "\n",
        "grouped[\"cosine_mean\"] = grouped[\"cosine_similarity\"].apply(lambda sims: np.mean(sims))\n",
        "grouped[\"nn_overlap\"] = grouped.apply(lambda row: nn_overlap(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "grouped[\"centroid_shift\"] = grouped.apply(lambda row: centroid_shift(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "grouped[\"dispersion_change\"] = grouped.apply(lambda row: dispersion_change(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "grouped[\"temporal_ref_sim\"] = grouped.apply(lambda row: temporal_ref_similarity(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "grouped[\"entropy_t1\"] = grouped[\"text1\"].apply(contextual_entropy)\n",
        "grouped[\"entropy_t2\"] = grouped[\"text2\"].apply(contextual_entropy)\n",
        "grouped[\"entropy_change\"] = grouped[\"entropy_t2\"] - grouped[\"entropy_t1\"]\n",
        "grouped[\"uot_shift\"] = grouped.apply(lambda row: uot_shift(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "d2KMrzzI3F5m"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def top_k_neighbors(embeddings, all_embeddings, k=10):\n",
        "    sims = cosine_similarity(np.vstack(embeddings), np.vstack(all_embeddings))\n",
        "    avg_sims = sims.mean(axis=0)\n",
        "    return np.argsort(-avg_sims)[:k]\n",
        "\n",
        "def rbo_score(list1, list2, p=0.9):\n",
        "    overlap = 0.0\n",
        "    for i, (a, b) in enumerate(zip(list1, list2), 1):\n",
        "        if a in list2[:i]:\n",
        "            overlap += 1 / i\n",
        "    return (1 - p) * overlap\n",
        "\n"
      ],
      "metadata": {
        "id": "e6tiW-b4Fg2Z"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_shift(embeddings_t1, embeddings_t2, n_clusters=2):\n",
        "    X1 = np.vstack(embeddings_t1)\n",
        "    X2 = np.vstack(embeddings_t2)\n",
        "    kmeans1 = KMeans(n_clusters=min(n_clusters, len(X1))).fit(X1)\n",
        "    kmeans2 = KMeans(n_clusters=min(n_clusters, len(X2))).fit(X2)\n",
        "    centroids1 = kmeans1.cluster_centers_\n",
        "    centroids2 = kmeans2.cluster_centers_\n",
        "    shift = np.mean([min(np.linalg.norm(c1 - c2) for c2 in centroids2) for c1 in centroids1])\n",
        "    return shift\n"
      ],
      "metadata": {
        "id": "x201QNwtFnG2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "def wasserstein_shift(embeddings_t1, embeddings_t2):\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=1)\n",
        "    all_embs = np.vstack(embeddings_t1 + embeddings_t2)\n",
        "    pca.fit(all_embs)\n",
        "    proj_t1 = pca.transform(np.vstack(embeddings_t1)).flatten()\n",
        "    proj_t2 = pca.transform(np.vstack(embeddings_t2)).flatten()\n",
        "    return wasserstein_distance(proj_t1, proj_t2)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def context_entropy(contexts):\n",
        "    counts = Counter()\n",
        "    for ctx in contexts:\n",
        "        tokens = ctx.split()\n",
        "        counts.update(tokens)\n",
        "    probs = np.array(list(counts.values())) / sum(counts.values())\n",
        "    return -np.sum(probs * np.log2(probs + 1e-12))\n",
        "\n",
        "def entropy_shift(texts_t1, texts_t2):\n",
        "    return context_entropy(texts_t2) - context_entropy(texts_t1)\n"
      ],
      "metadata": {
        "id": "2SczjhC8Fren"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "grouped[\"cluster_shift\"] = grouped.apply(lambda row: cluster_shift(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "grouped[\"wasserstein_shift\"] = grouped.apply(lambda row: wasserstein_shift(row[\"emb1\"], row[\"emb2\"]), axis=1)\n",
        "grouped[\"context_entropy_shift\"] = grouped.apply(lambda row: entropy_shift(row[\"text1\"], row[\"text2\"]), axis=1)\n"
      ],
      "metadata": {
        "id": "EIbyt49MFxV4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "metrics = [\"cosine_mean\", \"uot_shift\", \"wasserstein_shift\", \"context_entropy_shift\"]\n",
        "\n",
        "corr = grouped[metrics].corr()\n",
        "print(corr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Jhkgm-6oP1",
        "outputId": "d5cb2bf2-cd25-45d7-ad3e-ea283ff82f0d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       cosine_mean  uot_shift  wasserstein_shift  \\\n",
            "cosine_mean               1.000000   0.102114          -0.275696   \n",
            "uot_shift                 0.102114   1.000000          -0.147233   \n",
            "wasserstein_shift        -0.275696  -0.147233           1.000000   \n",
            "context_entropy_shift     0.104229  -0.211980          -0.274519   \n",
            "\n",
            "                       context_entropy_shift  \n",
            "cosine_mean                         0.104229  \n",
            "uot_shift                          -0.211980  \n",
            "wasserstein_shift                  -0.274519  \n",
            "context_entropy_shift               1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 3\n",
        "for m in metrics:\n",
        "    print(f\"\\nTop {top_n} words by {m}:\")\n",
        "    if m in [\"cosine_mean\", \"nn_overlap\", \"temporal_ref_sim\"]:  # similarity metrics\n",
        "        top_words = grouped.sort_values(m, ascending=True).head(top_n)\n",
        "    else:\n",
        "        top_words = grouped.sort_values(m, ascending=False).head(top_n)\n",
        "    print(top_words.index.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS_oUeBAOYwz",
        "outputId": "73b73b92-c0a9-473f-8110-3018ec990f7a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 3 words by cosine_mean:\n",
            "['folklore', 'epicenter', 'teargas']\n",
            "\n",
            "Top 3 words by uot_shift:\n",
            "['villager', 'turnip', 'pogrom']\n",
            "\n",
            "Top 3 words by wasserstein_shift:\n",
            "['entanglement', 'turnip', 'frisk']\n",
            "\n",
            "Top 3 words by context_entropy_shift:\n",
            "['ventilator', 'mask', 'frisk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "different metrics capture different types of shifts\n",
        "\n",
        "\n",
        "* Cosine similarity is largely independent from UOT, Wasserstein, and context entropy. This means average embedding shift often does not align with distributional or context diversity shifts.\n",
        "* UOT is capturing distributional/contextual shifts that other metrics mostly\n",
        "miss.\n",
        "* Context entropy aptures change in diversity of word contexts, which is mostly independent from embedding-based metrics.\n"
      ],
      "metadata": {
        "id": "EUk_OaWJ8FwX"
      }
    }
  ]
}